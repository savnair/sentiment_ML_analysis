{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35eb1748-d089-4b87-8751-65871d68a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32389a96-8b24-4a51-9014-b013197ba9f5",
   "metadata": {},
   "source": [
    "The Sentiment140 dataset has 1.6 million entries of tweets. Each tweet entry has the polarity of the tweet from negative to positive in terms of language sentiment (target), a tweet ID (ids), the date the tweet was posted (date), a query for the tweet if available (flag), the user that posted the tweet (user), and the actual tweet content in text format (text). \n",
    "\n",
    "In this project we are going to train models to predict the sentiment of a tweet based on its content, given this training dataset. Since most parameters in this dataset are not necessary for the training, we will be dropping all columns except the polarity parameter and the tweet content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb14a9b-479e-4863-8c58-d6f7e1c5e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = pd.read_csv(\"/Users/savitha/Desktop/New Yorkie University/2023-2024/Machine Learning/Homework/sentiment140.csv\", header=None, names=columns, encoding=\"ISO-8859-1\")\n",
    "\n",
    "df = df.drop([\"ids\", \"date\", \"flag\", \"user\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7860bca-1283-4218-b2bb-27daf4ebfce9",
   "metadata": {},
   "source": [
    "In this project, we are going to train our model to predict sentiment in a binary fashion. Since the given dataset is currently using values from 0 to 4 to show sentiment, we are going to change the value mappings to keep the max below 1. \n",
    "\n",
    "All tweets with a value of 0 or 1 should be negative, so they will be mapped to a value of 0. \n",
    "\n",
    "Values of 2 are netural tweets and will not help with our training since we are keeping the training binary, so the netural tweets will be dropped from the dataset. \n",
    "\n",
    "Finally, values of 3 and 4 indicate positive sentiment, so they will be mapped to a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c7e69a-ab89-41d1-9b65-cd37fc9b2401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map values to binary (0 = negative, 4 = positive)\n",
    "df[\"target\"] = df[\"target\"].apply(lambda x: 0 if (x >= 0 and x <= 1) else 1 if (x >= 3 and x <= 4) else None)\n",
    "\n",
    "# Drop rows with None values in the 'target' column (neutral tweets)\n",
    "df = df.dropna(subset=[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e294d83e-819a-45d2-9a27-95292914d06e",
   "metadata": {},
   "source": [
    "Since the input is raw text from the dataset of tweets and their content, the data will need to be translated to an understandable format for the models and their training. We will be using TF-IDF feature extraction for this, where raw text is translated into a numerical representative format. \n",
    "\n",
    "Stop words are exlcuded from the tweet content being fed to the model. Stop words are words that do not contribute to the overall understanding of a sentence. They include words such as \"the\", \"and\", or \"is\". \n",
    "\n",
    "The extracted content is then used for our modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaeeb719-945d-4c2a-8dc7-408548608e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "df = pd.concat([df, df_tfidf], axis=1)\n",
    "df = df.drop(\"text\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7717cd-2f13-4bdf-a329-3fe60743aaa2",
   "metadata": {},
   "source": [
    "At this point we have all we need for our training. We have the parsed content of the tweets and their corresponding sentiment in binary format. In order to split the data accordingly, we assign them to values X and Y, where X is the parsed features from the TF-IDF extraction and contain tweet content and Y is the binary sentiment values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e0c715-c26e-4b7c-82d8-5b87342858b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b0ee08-7197-44a0-bd76-265b4a059e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7edb774f-cc0f-4626-a247-c47dc6670cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisitic Regression Model\n",
      "\n",
      "Accuracy: 0.7405\n",
      "Precision: 0.7248\n",
      "Recall: 0.7780\n",
      "F1 Score: 0.7505\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "log_reg = LogisticRegression(random_state=999)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision = precision_score(y_test, y_pred_log_reg)\n",
    "recall = recall_score(y_test, y_pred_log_reg)\n",
    "f1 = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(\"Logisitic Regression Model\\n\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3db254b6-1e49-4d49-a955-3862db428715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Model\n",
      "\n",
      "Accuracy: 0.7449\n",
      "Precision: 0.7320\n",
      "Recall: 0.7754\n",
      "F1 Score: 0.7530\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier\n",
    "mlp_clf = MLPClassifier(random_state=999)\n",
    "mlp_clf.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_mlp)\n",
    "precision = precision_score(y_test, y_pred_mlp)\n",
    "recall = recall_score(y_test, y_pred_mlp)\n",
    "f1 = f1_score(y_test, y_pred_mlp)\n",
    "\n",
    "print(\"MLP Classifier Model\\n\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd820880-b025-4604-afc4-ef3650d8c0d9",
   "metadata": {},
   "source": [
    "Final Observations:\n",
    "\n",
    "From the results of the two models, we can see that both models performed at about the same level. The differences in the metrics of accuracy and the others were small and therefore negligible. An accuracy of around 74% is quite good, and was the result of both models. \n",
    "\n",
    "The logisitic regression model is a simple model and was mainly used for comparision of results to other model's training. However, the model performed relatively well. The MLP, or Multi-Layer Perceptron model, was used since it is a complex neural network model and would be good at recognizing more complex patterns in data, as would be the case for different words in the tweets of our dataset. \n",
    "\n",
    "However, it is important to note that while training my models, I had selected to pause the training of the MLP model after 7 hours as I had felt that the training was taking abnormally long. It was after this long training period that the metrics were calculated. Although the model had very slightly better metrics, I do not think the long training time was a good trade-off. Of course, this may have been due to the nature of the dataset as it is quite large at 1.6 million entries. \n",
    "\n",
    "Given the final results, the logistic regression model should be preferred as its metrics are on-par with the MLP model for a exponentially lower training period. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
